
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [随机种子](#随机种子)
  - [训练过程的不确定性](#训练过程的不确定性)
  - [随机种子的介绍](#随机种子的介绍)
  - [代码分析](#代码分析)
  - [CUDNN设置](#cudnn设置)
    - [CUDA convolution benchmarking => torch.backends.cudnn.benchmark](#cuda-convolution-benchmarking--torchbackendscudnnbenchmark)
    - [nondeterministic algorithms => torch.backends.cudnn.deterministic](#nondeterministic-algorithms--torchbackendscudnndeterministic)
    - [dataloader设置](#dataloader设置)

<!-- /code_chunk_output -->


# 随机种子
深度学习中，模型完成后运行代码，会发现每次运行出来的结果都不一样。这种情况会导致我们在原有的模型基础做出修改后而跑出的结果，
无法确定是改进了还是变得更差了。所以要固定随机种子来使每次运行的结果一样，以便比较不同模型的优劣。

## 训练过程的不确定性
在训练过程中，若相同的数据数据集，相同的训练集、测试集划分方式，相同的权重初始化，但是每次训练结果不同，可能有以下几个原因：
- PyTorch、Python、Numpy中的随机种子没有固定；
- cuDNN中大量nondeterministic的算法
- dataloader多个num_workers带来的随机性
- 向上采样和插值函数/类的向后是不确定的(PyTorch的问题)
- Dropout的存在(模型设计本身的问题)

例如，数据预处理、增强方式采用了概率，若没有设置固定的随机种子，结果可能不同。例如常用数据增强库albumentations就采用了Python的随机产生器；训练数据集被随机打乱了顺序，也可能用到 PyTorch、Python、Numpy 中的 shuffle，如果随机种子不固定，那么每次打乱顺序也不一样，导致最终结果不能复现。  

当向上采样和插值函数/类的向后是不确定的(BACKWARD of upsampling and interpolation functionals/classes is non-deterministic)。这意味着，如果你在训练图中使用这样的模块，无论你做什么，都永远不会得到确定性的结果。torch.nn.ConvTranspose2d函数是不确定的，除非你使用torch.backends.cudnn.deterministic = True (原文中说you can try to make the operation deterministic … by setting torch.backends.cudnn.deterministic = True，所以这样做是否能够得到正确结果也是待定的)。

## 随机种子的介绍
1. **随机种子是用于生成伪随机数序列的起始值**。在计算机中，所谓的"随机"实际上是通过特定的算法生成的伪随机序列。而随机种子就是这个算法的起始输入。  
2. 在很多编程语言和应用中，可以使用随机种子来控制伪随机数的生成。通过指定相同的随机种子，可以确保每次生成的随机数序列是一致的。这在某些需要可复现性的应用中非常有用，比如调试和测试。  
3. 随机种子可以是任何整数值。常见的做法是使用当前系统时间作为随机种子，因为时间通常是不断变化的，这样可以产生看似随机的序列。也可以手动指定一个固定的种子值，以产生确定性的随机数序列。  

**随机种子是一个起始值，用于生成伪随机数序列，指定相同的随机种子可以确保每次生成相同的随机数序列。**

```python
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # 在cuda 10.2及以上的版本中，需要设置以下环境变量来保证cuda的结果可复现
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark     = False
        torch.backends.cudnn.enabled       = True
```

## 代码分析
1. os.environ[‘PYTHONHASHSEED’] = str(seed)
主要是为了禁止 hash 随机化。

2. torch.manual_seed(seed)
与 torch.cuda.manual_seed(seed) 的功能是类似的，一个是设置当前 CPU 的随机种子，而另一个是设置当前 GPU 的随机种子，如果存在多个 GPU 可以使用 torch.cuda.manual_seed_all(seed) 对全部 GPU 都设置随机种子。

3. torch.backends.cudnn.deterministic = True
确定是否使用确定性卷积算法（默认是 False ），如果为 True，则能保证在相同设备上的相同输入能够实现相同输出。

4. torch.backends.cudnn.benchmark = False
如果设置 torch.backends.cudnn.benchmark = True 会让程序在一开始时增加额外的预处理时间，以让整个 model 的卷积层寻找到最适合的、最有效率的卷积实现算法，进而实现网络加速，但是与此同时可能会导致结果不可复现。  

5. torch.backends.cudnn.enabled = False  
torch.backends.cudnn.enabled: 这是PyTorch中的一个属性，用于控制是否启用CUDNN库。当设置为 True 时。如果环境中安装了CUDNN，并且PyTorch能够与之兼容，那么PyTorch将使用CUDNN来加速神经网络的运算。
torch.backends.cudnn.benchmark: 设置为 True 可以让 CUDNN 基准测试不同的算法，并选择最优的算法来执行操作，但这会增加一些性能开销。
torch.backends.cudnn.deterministic: 设置为 True 可以使得每次运算的结果是确定的，但可能会降低性能。
 
## CUDNN设置
cudnn中对卷积操作进行了优化，牺牲了精度来换取计算效率。
**GPU算法的不确定来源有两个：**
- CUDA convolution benchmarking（基准测试）
- nondeterministic algorithms

### torch.backends.cudnn.benchmark
CUDA convolution benchmarking 是为了提升运行效率，对模型参数试运行后，选取最优实现。

CUDA卷积操作使用的cuDNN库可能是跨应用程序多次执行的不确定性的来源。当使用一组新的尺寸参数调用cuDNN卷积时，一个可选的特性可以**运行多个卷积算法，对它们进行基准测试以找到最快的一个**。然后，在剩下的过程中，对于相应的尺寸参数集，将一致地使用最快的算法。

适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括 batch size，图片大小，输入的通道）是不变的。反之，如果卷积层的设置一直变化，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。

由于benchmarking本身存在噪音，即使是在同一台机器上benchmarking可能会在后续的运行中选择不同的算法，导致不确定性。     

**torch.backends.cudnn.benchmark=True ：**cuDNN使用非确定性算法寻找最高效算法。将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速、增加运行效率。
**torch.backends.cudnn.benchmark = False ：**禁用基准功能会导致cuDNN确定性地选择算法，可能以降低性能为代价。保证gpu每次都选择相同的算法，但是不保证该算法是deterministic的。

### torch.backends.cudnn.deterministic
GPU最大优势就是并行计算，如果能够忽略顺序，就避免了同步要求，能够大大提升运行效率，所以很多算法都有非确定性结果的算法实现。虽然禁用CUDA卷积基准（benchmarking ），确保每次运行时CUDA选择相同的算法应用程序,但算法本身可能是不确定的。通过设置deterministic就可以使得pytorch选择确定性算法。

torch.backends.cudnn.deterministic = True ：每次返回的卷积算法将是确定的，即默认算法。如果配合上设置Torch的随机种子为固定值的话，应该可以保证每次运行网络的时候相同输入的输出是固定的。     

**torch.use_deterministic_algorithms(true) 和 torch.backends.cudnn.deterministic = True 区别：**

torch.backends.cudnn.deterministic = True , **只设置控制选择确定性这种行为**，而
torch.use_deterministic_algorithms(true) 允许配置PyTorch，将使其他PyTorch操作的行为具有确定性，在可用的情况下使用确定性算法，而不是非确定性算法，如果操作已知为非确定性算法（且没有确定性替代方案），则会抛出错误。


## dataloader设置
如果dataloader采用了多线程(num_workers > 1), 那么由于读取数据的顺序不同，最终运行结果也会有差异。也就是说，改变num_workers参数，也会对实验结果产生影响。目前暂时没有发现解决这个问题的方法，但是只要固定num_workers数目(线程数)不变，基本上也能够重复实验结果。  
对于不同线程的随机数种子设置，主要通过DataLoader的**worker_init_fn**参数来实现。默认情况下使用线程ID作为随机数种子。
```python
def worker_init_fn(worked_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

Data.DataLoader(
    dataset    = xxx, 
    batch_size = xxx,  
    shuffle    = xxx,  
    # 关键是下面这两个，上面自己随便设置
    worker_init_fn = worker_init_fn,
    num_workers    = 0,
)

```
