# 深度学习笔记(1)



## 模型训练基本流程
```python
for data, target in train_loader:
    optimizer.zero_grad()           # 清除梯度
    output = model(data)            # 前向传播
    loss = loss_fn(output, target)  # 计算损失
    loss.backward()                 # 反向传播
    optimizer.step()                # 更新权重
 ```

```python
    class Module(object):
        def __init__(self):
        def forward(self, *input):
        def add_module(self, name, module):
        def cuda(self, device=None):
        def cpu(self):
        def __call__(self, *input, **kwargs):
        def parameters(self, recurse=True):
        def named_parameters(self, prefix='', recurse=True):
        def children(self):
        def named_children(self):
        def modules(self):  
        def named_modules(self, memo=None, prefix=''):
        def train(self, mode=True):
        def eval(self):
        def zero_grad(self):
        def __repr__(self):
        def __dir__(self):
```
在定义自已的网络的时候，需要继承nn.Module类，并且重载__init__和forward这两个方法。  
有一些注意技巧：  
（1）一般把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数__init__()中，也可以把不具有参数的层也放在里面；

（2）一般把不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)可放在构造函数中，也可不放在构造函数中，如果不放在构造函数
__init__里面，则在forward方法里面可以使用nn.functional来代替

（3）forward方法是必须要重写的，它是实现模型的功能，实现各个层之间的连接关系的核心。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```
self指的是实例Instance本身，在Python类中规定，函数的第一个参数是实例对象本身，类中的方法的第一个参数一定要是self，而且不能省略。
**Python中的super(Net, self).__init__() 是子类继承父类的__init__()的属性。** 
super(XXX, self).init()——对继承自父类的属性进行初始化，并且用父类的初始化方法初始化继承的属性。  


在用pytorch训练模型时，通常会在遍历epochs的过程中依次用到optimizer.zero_grad(),loss.backward()和optimizer.step()三个函数，如下所示：  
```python
model = MyModel()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)
 
for epoch in range(1, epochs):
    for i, (inputs, labels) in enumerate(train_loader):
        output= model(inputs)
        loss = criterion(output, labels)
        
        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```
总得来说，这三个函数的作用是先将梯度归零（optimizer.zero_grad()），然后反向传播计算得到每个参数的梯度值（loss.backward()），
最后通过梯度下降执行一步参数更新（optimizer.step()）

optimizer.zero_grad()函数会遍历模型的所有参数，通过p.grad.detach_()方法截断反向传播的梯度流，再通过p.grad.zero_()函数将每个参数的梯度值设为0，即上一次的梯度记录被清空。

因为训练的过程通常使用mini-batch方法，所以如果不将梯度清零的话，梯度会与上一个batch的数据相关，因此该函数要写在反向传播和梯度下降之前。