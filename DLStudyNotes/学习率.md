

### torch.optim.lr_scheduler.StepLR()函数
有的时候需要我们通过一定机制来调整学习率，这个时候可以借助于torch.optim.lr_scheduler类来进行调整；torch.optim.lr_scheduler模块
提供了一些根据epoch训练次数来调整学习率（learning rate）的方法。一般情况下我们会设置随着epoch的增大而逐渐减小学习率从而达到更好的训练效果。 

**class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)**

optimizer （Optimizer）：要更改学习率的优化器  
step_size（int）：每训练step_size个epoch，更新一次参数  
gamma（float）：更新lr的乘法因子  
last_epoch （int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。
默认为-1表示从头开始训练，即从epoch=1开始。

学习率衰减函数：每训练n个epoch，学习率衰减为原来的1/a  
```python
from torch import optim
optimizer_ft=optim.Adam(params_to_update,lr=1e-2)
scheduler=optim.lr_scheduler.StepLR(optimizer_ft,step_size=n,gamma=0.1)#学习率衰减，每训练n个epoch，学习率衰减为原来的1/a

initial_lr = 0.1
 
net_1=nn.Sequential(
    nn.Linear(1,10)
)
 
optimizer_1 = torch.optim.Adam(
    net_1.parameters(), 
    lr = initial_lr)
 
scheduler_1 = StepLR(
    optimizer_1, 
    step_size=3, 
    gamma=0.1)
 
print("初始化的学习率：", optimizer_1.defaults['lr'])
lst=[]
for epoch in range(1, 11):
    # train
    optimizer_1.zero_grad()
    optimizer_1.step()
    print("第%d个epoch的学习率：%f" % (epoch, optimizer_1.param_groups[0]['lr']))
    lst.append( optimizer_1.param_groups[0]['lr'])
    scheduler_1.step()
```



